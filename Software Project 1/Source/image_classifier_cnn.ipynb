{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '.data'\n",
    "\n",
    "train_data = datasets.MNIST(root = ROOT, \n",
    "                            train = True, \n",
    "                            download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIT_DEPTH = 8\n",
    "MAX_VALUE = pow(2, BIT_DEPTH) - 1\n",
    "\n",
    "mean = train_data.data.float().mean() / MAX_VALUE\n",
    "std = train_data.data.float().std() / MAX_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                            transforms.RandomRotation(5, fill=(0,)),\n",
    "                            transforms.RandomCrop(28, padding = 2),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean = [mean], std = [std])\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = [mean], std = [std])\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root = ROOT, \n",
    "                            train = True, \n",
    "                            download = True, \n",
    "                            transform = train_transforms)\n",
    "\n",
    "test_data = datasets.MNIST(root = ROOT, \n",
    "                           train = False, \n",
    "                           download = True, \n",
    "                           transform = test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_RATIO = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data, \n",
    "                                           [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transform = test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training examples: 54000\nNumber of validation examples: 6000\nNumber of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_iterator = data.DataLoader(valid_data, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "test_iterator = data.DataLoader(test_data, \n",
    "                                batch_size = BATCH_SIZE)"
   ]
  },
  {
   "source": [
    "    CNN1 = [1,28,28] -> [6,24,24] -> [6,12,12] -> [16,8,8] -> [16,4,4] -> 256 -> 120 -> 84 -> 10\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 10\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, \n",
    "                               out_channels = 6, \n",
    "                               kernel_size = 5)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, \n",
    "                               out_channels = 16, \n",
    "                               kernel_size = 5)\n",
    "\n",
    "        self.fc_1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc_2 = nn.Linear(120, 84)\n",
    "        self.fc_3 = nn.Linear(84, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size = 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, kernel_size = 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        h = x\n",
    "\n",
    "        x = self.fc_1(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_3(x)\n",
    "\n",
    "        return x, h\n",
    "\n",
    "\n",
    "model = CNN(OUTPUT_DIM)"
   ]
  },
  {
   "source": [
    "    CNN2 = [1,28,28] -> [2,20,20] -> [2,10,10] -> [32,8,8] -> [32,4,4] -> 512 -> 200 -> 64 -> 10"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 10\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, \n",
    "                               out_channels = 2, \n",
    "                               kernel_size = 9)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = 2, \n",
    "                               out_channels = 32, \n",
    "                               kernel_size = 3)\n",
    "\n",
    "        self.fc_1 = nn.Linear(512, 200)\n",
    "        self.fc_2 = nn.Linear(200, 64)\n",
    "        self.fc_3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size = 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, kernel_size = 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        h = x\n",
    "\n",
    "        x = self.fc_1(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_3(x)\n",
    "\n",
    "        return x, h\n",
    "\n",
    "\n",
    "model = CNN(OUTPUT_DIM)"
   ]
  },
  {
   "source": [
    "    CNN3 = [1,28,28] -> [32,28,28] -> [32,14,14] -> [64,14,14] -> [64,7,7] -> 3136 -> 128 -> 10"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 10\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, \n",
    "                               out_channels = 32, \n",
    "                               kernel_size = 3,\n",
    "                               padding = 1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, \n",
    "                               out_channels = 64, \n",
    "                               kernel_size = 3,\n",
    "                               padding = 1)\n",
    "\n",
    "        self.fc_1 = nn.Linear(3136, 128)\n",
    "        self.fc_2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = F.max_pool2d(x, kernel_size = 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = F.max_pool2d(x, kernel_size = 2)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        h = x\n",
    "\n",
    "        x = self.fc_1(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        return x, h\n",
    "\n",
    "\n",
    "model = CNN(OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 44,426 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred, _ = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred, _ = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1,1.344431795039267,0.6021928944677677,0.39730053146680194,0.8906101981798807\n",
      "2,0.47182117992976924,0.8568702789972413,0.19119842102130255,0.9455788334210714\n",
      "3,0.2709628197382081,0.9188551430432301,0.12968300779660544,0.9616062939167023\n",
      "4,0.20340423089153362,0.9399184719571527,0.11259398609399796,0.9664328793684641\n",
      "5,0.1735230377822552,0.9473640255208285,0.09150555233160655,0.9725349148114523\n",
      "6,0.14920124397525247,0.9561879792303409,0.08924427255988121,0.9723691940307617\n",
      "7,0.1363324260093131,0.958395927582147,0.08066091934839885,0.9752988815307617\n",
      "8,0.1300280754858593,0.9611311656124187,0.07648349304993947,0.9768998523553213\n",
      "9,0.11938929389107902,0.9633453870719334,0.08047020559509595,0.9761955440044403\n",
      "10,0.10703897223157703,0.9674990109677585,0.06402550265192986,0.9810221294562022\n",
      "11,0.09983321180883444,0.9693502075267288,0.06613363822301228,0.9799627065658569\n",
      "12,0.0956974282579602,0.9699986352110809,0.06361116593082745,0.9809954961140951\n",
      "13,0.09131351197665592,0.9716820390719287,0.06348266576727231,0.9794773856798807\n",
      "14,0.08453424696652394,0.9738194211474005,0.05235106684267521,0.9833007752895355\n",
      "15,0.07937109674204071,0.9753605161073073,0.05343232241769632,0.9834901690483093\n",
      "16,0.08154867027165755,0.9747073830298658,0.05467989047368368,0.9825668732325236\n",
      "17,0.07433997670996864,0.9775645440479495,0.049844225868582726,0.9837061961491903\n",
      "18,0.07404670674564703,0.9765715160459842,0.050094700728853546,0.9868755837281545\n",
      "19,0.07384734551580448,0.9771995589418231,0.05029203680654367,0.9845732649167379\n",
      "20,0.06981079617761216,0.9780240103883563,0.050684171418348946,0.9841382503509521\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "    # end_time = time.monotonic()\n",
    "\n",
    "    # epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    # print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print(f'{epoch+1},{train_loss},{train_acc},{valid_loss},{valid_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.04085181159898639,0.9868482947349548\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
    "\n",
    "# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "print(f'{test_loss},{test_acc}')"
   ]
  }
 ]
}